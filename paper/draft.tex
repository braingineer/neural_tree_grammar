\documentclass[11pt]{article}
\usepackage{coling2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{scrextend}
\makeatletter
\renewcommand*{\p@section}{\S\,}
\renewcommand*{\p@subsection}{\S\,}
\makeatother

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Syntactic realization with data-driven neural tree grammars}

% \author{Brian McMahan \\
%   Computer Science, Rutgers University \\
%   {\tt brian.mcmahan@rutgers.edu} \\\And
%   Matthew Stone \\
%   Computer Science, Rutgers University \\
%   {\tt matthew.stone@rutgers.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

A key component in surface realization in natural language generation
is to choose concrete syntactic relationships to express a target
meaning.
%
We develop a new method for syntactic choice based on learning a
stochastic tree grammar in a neural architecture.
%
This framework can exploit state-of-the-art methods for modeling word
sequences and generalizing across vocabulary.
%
Our methods also embed elementary tree structures using convolutional
coding to generalize over tree structures.
%
We evaluate the models on the task of linearizing unannotated
dependency trees, achieving improvements over previous methods.

\end{abstract}

\section{Introduction}

Where natural language understanding systems face problems of
ambiguity, natural language generation (NLG) systems face problems of
choice. A wide coverage NLG system must be able to formulate messages
using specialized linguistic elements in the exceptional circumstances
where they are appropriate; however, it can only achieve fluency by
expressing frequent meanings in routine ways. Empirical methods have
thus long been recognized as crucial to NLG; see
e.g. \newcite{langkilde1998generation}.

With traditional stochastic modeling techniques, NLG researchers have
had to predict choices using factored models with handcrafted
representations and strong independence assumptions, in order to avoid
combinatorial explosions and address the sparsity of training data.
By contrast, in this paper, we leverage recent advances in deep
learning to develop new models for syntactic choice that free
engineers from many of these decisions, but still generalize more
effectively and match human choices more closely than traditional
techniques.

We adopt the characterization of syntactic choice from
\newcite{bangalore2000exploiting} which uses a stochastic tree model and a language model to produce a linearized string from an unordered,
unlabeled dependency graph.
%
The first step to producing a linearized string is
to assign each node an appropriate supertag---a fragment of a parse tree with
a leaf left open for a lexical item---using a stochastic tree model.
%A language model is then used to select among the many possible derivation t
The resulting assignments are consistent with many possible derivation trees
because of adjunct ambiguitity.
%
To finish the linearization, a language model is used to select among them.
%
While improving the language model would improve the linearized string,
this work focuses on more accurately predicting the correct supertags from
unlabeled dependency trees.

In this work, we make two novel contributions to improve upon the stochastic tree model:
a supertag embedding technique which uses convolutional neural networks and
a recurrent tree network which can model hierarchical relationships.
%
By embedding supertags and using a recurrent tree network for the relationships between them,
we enable more information to be used at each node.
%
We evaluate our contributions in two ways: first, by varying the technique used to embed
supertags, and then by comparing a feed-forward model against our
recurrent tree model.


Our presentation begins in \ref{sec:tree} with an introduction to tree
grammars and a deterministic metholodogy for inducing the elementary trees of the grammar.
%
Next, \ref{sec:neural} presents the techniques we have developed to represent
a tree grammar using a neural architecture.
%
Then, in \ref{sec:models}, we describe the specific model we have
implemented, and in \ref{sec:algo}, we describe the algorithms used to describe the classification results.
%
The experiments in \ref{sec:expt} demonstrate the improvement of the
model over baseline results based on previous work on stochastic
surface realization.
%
We conclude with a brief discussion of the future potential for neural
architectures to predict grammatical choices in NLG.

\section{Tree Grammars}
\label{sec:tree}

Broadly, tree grammars are a family of tree rewriting formalisms that
modify structure to produce strings.  The atomic syntactic units are
called elementary trees; elementary trees combine using tree-rewrite
rules to form derived phrase structure trees describing complex
sentences.  Inducing a tree grammar involves fixing a formal inventory
of structures and operations for elementary trees and then inferring
instances of those structures to match corpus data.

\subsection{The Grammar Formalism}

The canonical tree grammar is perhaps lexicalized tree-adjoining
grammar (LTAG) \cite{joshi1991tree}.
%
The elementary trees of LTAG consist of two disjoint sets with distinct operations
%%%
The elementary trees of LTAG are disjoint sets based on the operations
they can perform: initial trees can perform substitution operations
and auxiliary trees can perform adjunction operations.
%
The substitution operation replaces a non-terminal leaf of a target
tree with an identically-labeled root node of an initial tree.
%
The adjunction operation modifies the internal structure of a target
tree at by expanding a node identically-labeled with the root and a
distinguished foot note in the auxiliary tree.
%
The lexicalization of the the grammar requires each elementary tree to
have at least one lexical item as a leaf.

LTAG incurs computational costs because it is mildly context-sensitive
in generative power.
%
Several variants reduce the complexity of the formalism by limiting
the range of adjunction operations.
%
For example, the Tree Insertion Grammar allows for adjunction as long as it is either a left or right auxiliary tree \cite{Schabes1995}.
%
Tree Substitution Grammars, meanwhile, allow for no adjunction and
only substitutions \cite{cohn2009inducing}.

We adopt one particular restricted adjunction, called
sister-adjunction or \emph{insertion}, which allows trees to attach to
an interior node and add itself as a first or last child
\cite{chiang2000statistical}.
%
Chiang's sister-adjunction allows for the flat structures in
the Penn Treebank while limiting the formalism to context-free power.

\subsection{Inducing a tree grammar}

In lexicalized tree grammars, the lexicon and the grammatical rules
are one and the same.
%
The set of possible grammatical moves which can be made are simultaneously the set of possible words which can be used next.
%
This means inducing a tree grammar from a dataset is a matter of inferring the set of constructions in the data.

%%%%\tabularnewline

We follow previous work in using bracketed phrase structure corpora
and deterministic rules to induce the grammar
\cite{chiang2000statistical,bangalore2001impact}.
%
Broadly, the methodology is to split the observed trees into the
constituents which make it up, according to the grammar formalism.
%
We use head rules
\cite{chiang2000statistical,collins1997three,magerman1995statistical}
to associate internal nodes in a bracketed tree with the lexical item
that owns it.
%
We use additional rules to classify some children as complements,
corresponding to substitution sites and root notes of complement
trees; and other children as adjuncts, corresponding to insertion
trees that combine with the parent node, either to the right or to the
left of the head; this allows us to segment the tree into units of
substitution and insertion.%
\footnote{One particular downside of deterministically constructing
  the grammar this way is that it can produce an excess of superfluous
  elementary trees.
%
We minimize this by collapsing repeated projections in the treebank.
%
Other work has provided Bayesian models for reducing grammar
complexity by forcing it to follow Dirichlet or Pitman-Yor processes
\cite{Cohn2010}---an interesting direction for future work.}



\begin{figure*}[tH!]
\centering
\includegraphics[width=\textwidth]{spineembed.pdf}
\caption{Embedding supertags using convolutional neural networks. In (A), a tree is encoded by its features and then embedded.
 In (B), convolutional layers are used to encode the supertag into a vector.}
 \label{fig:spineembedding}
\end{figure*}

\section{Neural Representations}
\label{sec:neural}

%There is a tension between vectorized representations and dynamic, nonlinear data structures.
%
%The tension is emphasized by requirement of specialized hardware that both dramatically increases the data throughput and reduces the space of representable models\footnote{Specifically, to run on Graphical Processing Units (GPUs), model structure must be known at compile time or incur heavy computational costs.}.
%
This work makes two contributions to improve stochastic tree modeling with neural networks.
%This work makes two contributions to reduce that tension and progress methodologies for representing nonlinear data structures in linear spaces.
%
First, supertags are represented as vectors through convolutional coding,
opening up the recent repertoire of vector space techniques.
%
Second, hierarchical dependence between nodes are modeled using a recurrent tree
network.
%
%recurrent neural network variant that isn't restricted to conditioning hidden states on their temporal predecessors.
%In brief, this technique is made possible through recent advanced in StackLSTMs \cite{dyer2015transition} and the use of topological indexing of recurrent states.% hidden states of


\subsection{Embedding Supertags}

%The first step to embedding supertags with a convolutional neural network is to
%embed each of its nodes and combine them to form a tensor.
%
%
%To embed a supertag, we first embed all of its nodes to form a tensor and then use a
%To embed a supertag and summarize it into a vector, each of its nodes are
%first embedded, combined to form a tensor, and then
%We embed supertags by embed each of its nodes, combine
To embed a supertag, each of its nodes are embedded, grouped to form a tensor, and then summarized into a single vector using a series of convolutional neural networks.
%
Since the weights of a convolutional neural network are optimized during training, the data should determine which parts of the supertag are important.
%
%The purpose of convolutional coding is to allow the data to determine which facets of a supertag are relevant and useful.
%
The more usual alternative approach is to treat each supertag as the atomic
unit---essentially a statistical singleton---and map it either to a feature or
an embedding vector.
%
We evaluate our contribution and compare these two approaches to get a sense for whether convolutional coding robustly generalizes across supertags.


The goal of embedding supertags is to summarize the tree structure into a single vector.
%
Inside the tree structure, there are two separate pieces of information which need to be summarized:
the syntactic category of each node and the role each node plays in the tree grammar.
%
The root receives a special role---either substitution or insertion to represent
the supertag's tree operation.
%g
The remaining nodes either have the substitution point role or the spine role---they are along the spine from root to the lexical attachment point.


The nodes of a supertag can be independently embedded and combined to form a
tensor of embeddings.
%
Specifically, symbols representing the syntactic category and node roles are treated as
distinct vocabulary tokens, mapped to integers, and used to retrieve a vector
representation that is learned during training.
%
The vectors are grouped into a tensor by placing the root node into the
first cell of the first row and left-aligning the decendents in the subsequent rows.
%
The two tensors are combined by concatenating along the embedding dimension.
%
This embed-and-group method is shown in on the left in Figure \ref{fig:spineembedding}.
%the root node is in first cell in the first row and the subsequent nodes are


Using a series of convolutional neural networks which learn their weights during training, the tensor of embeddings can be reduced to a single vector.
%
To reduce the tensor to a vector, the convolutions are designed with increasingly larger filter sizes.
%The convolutions are applied with increasingly larger filter sizes, leading to a reduction in dimensions.
%
Additionally, the dimensions are reduced alternatingly to also facilitate the capture of features.
%
The entire process is summarized in Eq. \ref{eq:embed} with $\Lambda$ representing the supertags, $G$ representing embedding matrices, and
$C$ representing the convolutional neural network layers.
%
Specifically, $G_s$ is the syntactic category embedding matrix and
$G_r$ is the node role embedding matrix.
%
Each convolutional layer $C$ is shown with its corresponding height and width as $C^{i,j}$.
%
The encoding first constructs the tensor, $T_\Lambda$, through the embed-and-group method.
%
Then, the embedding matrix $G_\Lambda$ is summarized from $T_\Lambda$ using the
series of convolutional layers.

\begin{align}
& T_\Lambda = [G_s(\Lambda_{syntactic~category});~G_r(\Lambda_{role})] \nonumber \\
& G_\Lambda = C^{4,5}(C^{3,1}(C^{1,3}(C^{2,1}(C^{1,2}(T_\Lambda))))) \label{eq:embed}
\end{align}



The final product, a vector per supertag, is aggregated with the other vectors
and turned into an embedding matrix.
%
This is visualized in on the right Figure \ref{fig:spineembedding}.
%
During training and test time, supertags are input as indices and their feature
representations retrieved as an embedding.
%
Importantly, the convolutional layers are connected to the computational graph during training, so the parameters are optimized with respect to the task.


%%%%
\subsection{Recurrent Tree Networks}
\label{subsec:rtn}

\begin{figure*}[tH!]
\centering
\includegraphics[width=\textwidth]{rtn.pdf}
\caption{A recurrent tree network. (A) The dependency structure as a tree.  (B) the dependency structure as a sequence.}
 \label{fig:rtn}
\end{figure*}

To model the relationship between the supertags in the derivation tree, we use
a recurrent tree network.
%
While recurrent neural networks normally use the previous hidden state in the sequential order of the inputs,
%require  the hidden state to be from the
%temporally previous hidden state,
recurrent tree networks use the hidden state from the parent.
%
Utilizing the parent's hidden state rather than the sequentially previous hidden
state, the recurrent connection can travel down the branches of a tree.
%
An example of a recurrent tree network is shown in \ref{fig:rtn}.



In our recurrent tree network, child nodes gain access to a parent's hidden state
through an internal \emph{tree state}.
%
During a tree recurrence, the nodes in the dependency graph are enumerated in a
top-down traversal.
%
At each step in the recurrence, the resulting recurrent state is stored in the
tree state at the step index.
%
Descendents access the recurrent state using a topological index that is passed in as data.

The formulation is summarized in Equation \ref{eq:rnnsetup}.
%
The input to each time step in the current tree is the data, $x_t$, and a topological
index, $p_t$.
%
The recurrent tree uses $p_t$ to retrieve the parent's hidden state, $s_p$, from the
tree state, $S_{tree}$, and applies the recurrence function, $g(\dot)$.
%
The resulting recurrent state is the hidden state for child node, $s_c$.
%
The recurrent state $s_c$ is stored in the tree state, $S_{tree}$, at index $t$.

\begin{align}
       s_c~&=~RTN(x_t,p_t) \nonumber \\
           &=~g(x_t,S_{tree}[p_t]) \nonumber \\
           &=~g(x_t, s_p) \nonumber \\
S_{tree}[t]&=~s_c \label{eq:rnnsetup}
\end{align}

The use of topological indices allows for many recurrent tree networks to be
run in parallel on a GPU, increasing the efficiency of the implementation.
%
The primary concern for running parallel GPU computations is homogeneity because
the same operation will be applied to the entire data structure.
%
Normally, tree operations require flow control, making homogeneity impossible.
%
However, using topological indices and a tree state eliminates the need for flow control
by creating locally linear, homogenous computations.

\section{Models}
\label{sec:models}

In this work, we present two stochastic tree models which represent different modeling decisions.
%
The first, Fergus-N\footnote{Fergus-N is shorthand for Fergus-Neuralized}, is a
reimplementation of \newcite{bangalore2000exploiting}'s original FERGUS model but
implemented in a feed-forward neural network.
%
The second model, Fergus-R\footnote{Fergus-R is shorthand for Fergus-Recurrent}, utilizes a recurrent tree network.
%
Fergus-N serves as a baseline model to which we compare the recurrent tree network.


For both stochastic tree models, a recurrent neural network language model is used to complete the linearization task.
%Both models produce partial results which a recurrent neural network language model is used to complete the linearization task.
%
%It is important to note that it is the same language model
%By using the same language model to finishing linearizing the output of both models, any observed differences in performance can be localized to the performance of the stochastic tree models.
%
The same language model is to eliminate the confound of
language model performance and measure performance differences in the
stochastic tree modeling.
%

\subsection{Model 1: Fergus-N}

Fergus-N is a stochastic tree model which uses local parent-child information as inputs to a feed-forward network.
%
Each parent-child pair are treated as independent of all other pairs.
%
The probability of the parent's supertag is computed using an embedding of the pair's lexical material and an embedding of the child's supertag.
%
During training, the negative log probability of the observed parent supertags is minimized for each parent-child pair.
%
This objective function is formally defined in Eq. \ref{eq:fergusn_obj} where $tag_p$ is the parent supertag, $tag_c$ is the child supertag, $lex_p$ is the parent's lexical material, and $lex_c$ is the child's lexical material.
%
Note that the probability of supertags for the leaves of the tree are computed
with respect to their parent's lexical material.

\begin{align}
&min_{\theta} -[\sum_p\sum_{p\to c} log[P_\theta(tag_{p} | lex_{p}, lex_{c}, tag_{c})] + \sum_c log[P_\theta(tag_{c} |lex_{p}, lex_{c})]] \label{eq:fergusn_obj}
\end{align}


The model is implemented as a feed-forward neural network that computes a probability distribution over parent supertags conditioned on lexical material and the child's supertag.
%
Equation \ref{eq:fergusn} details the model formulation.
%
The lexical material, $lex_p$ and $lex_c$, are embedded using the word embedding matrix, $G_w$, concatenated, and mapped to a new vector, $\omega_{lex}$, with a fully connected layer, $FC_1$.
%
The child supertag, $tag_c$, is embedded with $G_\Lambda$ and concatenated the lexical vector, $\omega_{lex}$, forming an intermediate vector representation of the node, $\omega_{node}$.
%
The node vector is repeated for each of the parent's possible supertags, $tagset_p$, and then concatenated with their embeddings to construct the set of treelet vectors, $\Omega_{treelet}$.
%
The vector states for the leaf nodes are similarly constructed, but instead combine the lexical vector, $\omega_{lex}$ with the embeddings of the child's possible supertags, $tagset_c$.
%
The final operation induces a probability distribution over the treelet and leaf vectors using a score computed by the vectorized function, $\Psi_{predict}$, as the scalar in a softmax distribution. 

\begin{align}
&\omega_{lex} = FC_1([G_w(lex_p); G_w(lex_c)]) \label{eq:fergusn} \\
&\omega_{node}=concat([G_\Lambda(tag_c);~\omega_{lex}]) \nonumber \\
&\Omega_{treelet} = concat([repeat(\omega_{node}),~G_\Lambda(tagset_p)]) \nonumber \\
&\Omega_{leaf} = concat([repeat(\omega_{lex}),~G_\Lambda(tagset_c)]) \nonumber \\
&P_\theta(tag_{p,i} | lex_{p}, lex_{c}, tag_{c})=
\frac{exp(\Psi(\omega_{treelet_i})))}
{\sum_{j \in |tagset_p|} exp(\Psi(\omega_{treelet_j})))} \nonumber \\
&P_\theta(tag_{c,i} |lex_{p}, lex_{c}) = 
\frac{exp(\Psi(\omega_{leaf_i})))}
{\sum_{j \in |tagset_c|} exp(\Psi(\omega_{leaf_j})))} \nonumber 
\end{align}


A decoding step uses the probability distributions to compute a high probability assignment for all supertags simueltaneously.
%
There are two primary difficulties that arise in this computation.
%
First, the conditional relationship of parents on children implies that the probability for the root supertag depends on the supertags of the entire tree.
%
Second, while it is easy to maintain local consistency---matching the syntactic
category of the child to an appropriate node on the parent---two children may choose substitution supertags which assume attachment to the same position on the parent.

To efficiently decode the supertag classifications, we implement an A* algorithm to incrementally select consistent supertag assignments.
%
At each step, the algorithm uses a priority queue to select subtrees based on their inside-outside scores.
%
The inside score is computed as the sum of the log probabilities of the supertags in the subtree.
%
The outside score is the sum of the best supertag for nodes outside the subtree, similar to \newcite{lewis2014improved}.
%
Once selected, the subtree is attached to the possible supertags of its parent that are both locally consistent and consistent among its already attached children.
%
These resulting subtrees are placed into the priority queue and the algorithm iterates to progress the search.
%
The search concludes either when a single complete tree has been found\footnote{Although, the data has some noise so that sometimes there is no complete tree that can possibly be formed}.


\subsection{Model 2: Fergus-R}

Fergus-R is a stochastic tree model implemented in a top-down recurrent tree network and augmented with soft attention.
%
For each node in the input dependency tree, soft attention---a method which learns a vectorized function to weight a group of vectors and sum into a single vector---is used to summarize its children.
%

The advantage of using a recurrent tree is that nodes are able to be informed of their ancestors.
%
Each node is further informed using soft attention over its children ---a method which learns a vectorized function to weight a group of vectors and sum into a single vector.
%
The soft attention vector and the node's embedded lexical material serve as the input to the recurrent tree.
%
The output of the recurrent tree represents the vectorized state of each node and is combined with each node's possible supertags to form prediction states.
%
%The recurrent tree output and the embedded supertag of the node's parent are used to compute a probability distribution over supertags.
%
%Using only summarized lexical information from its ancestors and children, the model computes a probability distribution over supertags for each node's recurrent tree output.
%
%so that the soft attention and recurrence relationships are only communicating lexical information.
Importantly, removing the conditional dependence on descendents' supertags
results in the simplified objective function in Eq. \ref{eq:rtnobj} where $lex_C$ is the children's lexical information, $lex_p$ is the parent's lexical information, $tag_p$ is the supertag for the parent node, and $RTN$ is the recurrent tree network. 

\begin{align}
&min_{\theta} -[\sum_{(p,C)} P_\theta(tag_{p}|RTN,~lex_p,~lex_{C})] \label{eq:rtnobj}
\end{align}

The Fergus-R model uses only lexical information as input to calculate the probability distribution over each node's supertags. 
%
The specific formulation is detailed in Eq. \ref{eq:fergusr}.
%
First, a parent node's children, $lex_C$, are embedded using the word embedding matrix, $G_w$, and then summarized with an attention function, $\Psi_{attn}$, to form the child context vector, $\omega_{C}$. 
%
The child context is concatenated with the embedded lexical information of the parent node, $lex_p$, and mapped to a new vector space with a fully connected layer, $FC_1$, to form the lexical context vector, $\omega_{lex}$.
%
The context vector and a topological vector for indexing the internal tree state (see \ref{subsec:rtn}) are passed to the recurrent tree network, $RTN$, to compute the full state vector for the parent node, $\omega_{node}$.
%
Similar to Fergus-N, the state vector is repeated and concatenated with the vectors of the parent node's possible supertags, ${tagset}_p$, and mapped to a new vector space with a fully connected layer, $FC_2$.
%
A vector in this vector space is labeled $\omega_{elementary}$ because the combination of supertag and lexical item constitutes an elementary tree.
%
The last step is to compute the probability of each supertag using the vectorized function, $\Psi_{predict}$.

\begin{align}
&\omega_{C} = \Psi_{attn}(G_w(lex_C)) \label{eq:fergusr} \\
&\omega_{lex} = FC_1(concat(\omega_{C},~G_w(lex_p))) \nonumber \\
&\omega_{node} = RTN(\omega_{lex},~topology) \nonumber \\
&\Omega_{elementary} = FC_2(concat(repeat(\omega_{node}),~G_\Lambda(tagset_p))) \nonumber \\
&P_\theta(tag_{p,i}~|~RTN,~lex_p,~lex_{C}) = 
\frac{exp(\Psi_{predict}(\omega_{elementary_i})))}
{\sum_{j \in |\Omega|} exp(\Psi_{predict}(\omega_{elementary_j))}} \nonumber
\end{align}

Although the same A* algorithm from Fergus-N is used, the decoding for Fergus-R is far simpler.
%
As supertags are incrementally selected in the algorithm, the inside score of the subsequent subtree is computed.  
%
Where Fergus-N had to compute a incremental dynamic program to evaluate the inside score, Fergus-R decomposes into a sum of conditionally independent distributions. 
%
The resulting setup is a chart parsing problem where the inside score of combining two consistent (non-conflicting) edges is just the sum of their inside scores. 

\subsection{Linearization}

The final step to linearizing the output of Fergus-N and Fergus-R---a dependency tree annotated with supertags and partial attachment information---is a search over possible orderings with a language model. 
%
There are many possible orderings due to adjunction operation
What remains to be determined is the order in which adjuncts are attached. 
%
Following \newcite{bangalore2000exploiting}, a language model is used to select between the alternate orderings. 
%
The language model used is a two-layer LSTM trained using the Keras library on
the surface form of the Penn Treebank.
%
The surface form was minimally cleaned\footnote{With respect to the surface form, the only
cleaning operations were to merge proper noun phrases into single tokens.  Punctuation and other
common cleaning operations were not performed.} to simulate realistic scenarios.

The difficulty of selecting orderings with a language model is that the possible linearizations can grow exponentially.
%
In particular, our implementations result in a large amount of insertion trees\footnote{Many of the validation examples had more than $2^{40}$ possible linearizations.}.
%
We approach this problem using a prefix tree which stores the possible linearizations as back-pointers to their last step and the word for the current step. 
%
The prefix tree is greedily searched with 32 beams.




\section{Experiments}
\label{sec:expt}

In this work, we make two contributions to stochastic tree modeling and evaluate them on a supertag prediction and linearization task. 
%
The evaluation uses the Wall Street Journal sections of the Penn Treebank which have been previously used for evaluating statistical tree grammars \cite{chiang2000statistical}\footnote{A possible additional data source, the data from the 2011 Shared Task on Surface Realization, was not available}.
%
Our data pipeline transforms the treebank into a collection of elementary trees and derivation tree---the structured relationships required to compose them into the observed data (see \ref{}).
%
Abstracting syntactic information from the derivation tree structure leads to the unlabeled dependency trees our models assume as input. 

We evaluate the convolutional coding of the supertags by comparing it against standard embedding techniques.
%
Specifically, we construct a matrix that has as many rows as supertags and is initialized with a Glorot uniform distribution.
%
Fergus-N and Fergus-R are retrained with the exact same parameters, except that instead of the supertag indices referencing a matrix constructed by a series of convolutions, it references our constructed embedding matrix (which gets optimized during learning). 
%
The impact of the embedding will be different for each model---Fergus-N uses embedded supertags in constructing the node state and Fergus-R does not---but any differences between the models and their alternate embedding counterparts can be perceived as evidence for either embedding technique.  


To evaluate whether a recurrent tree network can capture hierarchical and long-distance relationships, we compare it against a feed-forward model. 
%
Specifically, for each model, we measure how often it predicts the correct supertag or if it predicts a different supertag but with the correct root syntactic category, root node role, or lexical leaf node\footnote{The leaf to which a lexical item would attach} syntactic category.
%
These four quantities represent different aspects that are important to get right in a stochastic tree model.


%Fergus-R's predictions never observe its neighboring nodes' supertags, so any predictive improvements are based solely on lexical and ancestral context.

We also compare the differences in supertag embeddings and stochastic tree models on the linearization task itself.
%
The linearization tasks licenses more freedom in supertag classifications because it may have the same root and leaf syntactic categories but differ in minor constructions, such as number of arguments. 
%
The freedom allows for models to be more generalized and less fit to the specific supertags, but at the same time, it also mutes the distinctions between classification decisions.
%
The metrics used in the linearization task were taken from previous work, who argue that it correlates with generation performance better than n-gram counting methods. 


\subsection{Results}

Table \ref{table:accresults} shows the results of supertag classification using the stochastic tree models is shown. 
%
The scores are grouped in first by the model and then by the embedding method. 
%
The accuracy of each model is additionally broken down by how often it got the various components correct. 
%
The root category and role refer to the syntactic label and grammatical role of a supertag. 
%
The leaf category refers to the syntactic label of the leaf to which a lexical item would attach---essentially, this is the part of speech of the lexical item.
%
All differences between model are significant using a Paired-Sample t-test ($p<10^{5}$)

The Fergus-N model out-performed the Fergus-R model on supertag classification with convolutional supertag embeddings, but the Fergus-R model performed much closer to that of Fergus-N on the individual components. 


\begin{figure}
\centering
\begin{tabular}{|l|p{2cm}|p{2cm}|p{2.3cm}|p{2cm}|p{2.3cm}|}
\hline
Model & Embedding & Supertag & Root Category & Root Role & Leaf Category  \\ \hline
Fergus-N &  Convolution &  57.4\% & 81.8\% & 81.0\% & 86.8\% \\ \cline{2-6}
         &  Token       &  XX & XX & XX & \\ \cline{2-6}
\hline
Fergus-R &  Convolution &  36.4\% & 76.9\% & 78.3\% & 80.8\% \\ \cline{2-6}
         &  Token       &  XX & XX & XX & \\ \cline{2-6}
\hline
\end{tabular}
\label{table:accresults}
\caption{Classification accuracy by supertag and its individual properties}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|l|p{2cm}|p{1.5cm}|r|}
\hline
Model & Embedding & Beam Type & Accuracy \\ \hline
FERGUS & n.a. & n.a. & 74.9 \% \\
\hline \hline
Fergus-N & Convolution & $Top_{all}$  & 68.9\% \\ \cline{3-4}
         &             & $Best_{all}$ & 76.4\% \\ \cline{2-4}
         & Token       & $Top_{all}$  & XX\% \\ \cline{3-4}
         &             & $Best_{all}$ & XX\% \\ \cline{3-4}
\hline
Fergus-R & Convolution & $Top_{all}$  & 71.9\% \\ \cline{3-4}
         &             & $Best_{all}$ & 78.9\% \\ \cline{2-4}
         & Token       & $Top_{all}$  & XX \% \\ \cline{3-4}
         &             & $Best_{all}$ & XX \% \\ \cline{3-4}
\hline
\end{tabular}
\label{table:linresults}
\caption{Shown above as accuraccy is the percentage of tokens in the linearized strings that are in correct positions according to an edit distance measure.}
\end{figure}

\section{Related Work}
\label{sec:relatedwork}

There are several lines of related work which explore stochastic tree models, ranging exploration with recurrent neural networks to agglomerative pairwise associations. 
%
Most similar to ours, is the top down tree structure of \newcite{zhang2016top}.
%
In this model, first, two long short-term memory (LSTM) networks are used to make branch exploration decisions, and then an additional two LSTM networks are used to continue along these branches. 
%
The work of \newcite{Tai2015} similarly use an LSTM, but instead work from the leaves upward to merge the hidden states of children inside the recurrent step. 
%
The upward merging of children to form representations for parents has also been studied as pairwise agglomerations using recursive neural networks \cite{Socher2010}.

A growing body of work investigates modeling long distance relationships using efficient stacks in modern neural architecture. 
%
\newcite{dyer2015transition} utilize an indexable LSTM as a neural stack for transition-based parsing.  
%
Similarly, \newcite{bowman2016fast} utilize both a stack and buffer for sentence comprehension by utilizing an indexable structure and performing computations with indices to these structures. 
%
Finally, recent work models long distance relationships using the dynamic composability in ApolloCaffe, a fork of the Caffe library \cite{jia2014caffe}, to build different network structures based on parse trees \cite{Andreas2016LearningTC}.

\section{Conclusion}
\label{sec:conclusion}

While many neural models of grammar have focused on parsing, there has been relatively less attention on generation.
%
This is in large part due to the different kinds of decisions that have to be made in order for generation to work. 


The models outlined in this paper push forward methodologies for modeling tree grammars in neural networks. 


A lot of work has to be done to take advantage of deep learning and translate that to tasks beyond parsing, especially language production tasks.

This is a first step in a larger research program of stochastic generation models with the kind of generalization that deep learning techniques like word embeddings and recurrent neural networks give you. 



\bibliography{coling2016}
\bibliographystyle{acl}

\appendix

Model specifications
- layers, learning, etc

Head to supertag mapper


The data used in our experiments was taken from the Wall Street Jounral sections of the Treebank corpus.  

For the unlabeled dependency trees, we used the derivation structures which resulted from the deterministic grammar induction (see \ref{}). 

For the Fergus-N and Fergus-R learning problems, we processed the data into their relevant tuples.

Supertags were preprocessed into two data structures: one for the convolutional coding and one to map lexical items to supertags. 
%
The supertag data structure used for convolutional coding is a three-dimensional tensor where the first dimension is the supertag index, the second dimension is depth from root, and the third is relative sibling index. 
%
The tokens for syntactic category and node role are mapped to indices, stored in the data structure, and the data structure is stored in GPU memory.

During 


in model intro:
    - learning parameters in appendix
    - used keras + theano

in conv coding maybe:
    - gpu data structure

in experiments
    - where the data comes from


\end{document}
