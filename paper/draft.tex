\documentclass[11pt]{article}
\usepackage{coling2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{scrextend}
\makeatletter
\renewcommand*{\p@section}{\S\,}
\renewcommand*{\p@subsection}{\S\,}
\makeatother


%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Syntactic realization with data-driven neural tree grammars}

\author{Brian McMahan \and Matthew Stone \\
   Computer Science, Rutgers University \\
   {\tt brian.mcmahan, matthew.stone@rutgers.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}

A key component in surface realization in natural language generation
is to choose concrete syntactic relationships to express a target
meaning.
%
We develop a new method for syntactic choice based on learning a
stochastic tree grammar in a neural architecture.
%
This framework can exploit state-of-the-art methods for modeling word
sequences and generalizing across vocabulary.
%
We also induce embeddings to generalize over elementary tree
structures and exploit a tree recurrence over the input structure to
model long-distance influences between NLG choices.
%
We evaluate the models on the task of linearizing unannotated
dependency trees, documenting the contribution of our modeling
techniques to improvements in both accuracy and run-time.

\end{abstract}

\section{Introduction}

Where natural language understanding systems face problems of
ambiguity, natural language generation (NLG) systems face problems of
choice. A wide coverage NLG system must be able to formulate messages
using specialized linguistic elements in the exceptional circumstances
where they are appropriate; however, it can only achieve fluency by
expressing frequent meanings in routine ways. Empirical methods have
thus long been recognized as crucial to NLG; see
e.g. \newcite{langkilde1998generation}.

With traditional stochastic modeling techniques, NLG researchers have
had to predict choices using factored models with handcrafted
representations and strong independence assumptions, in order to avoid
combinatorial explosions and address the sparsity of training data.
%
By contrast, in this paper, we leverage recent advances in deep
learning to develop new models for syntactic choice that free
engineers from many of these decisions, but still generalize more
effectively, match human choices more closely, and enable more
efficient computations than traditional techniques.

We adopt the characterization of syntactic choice from
\newcite{bangalore2000exploiting}, which uses a stochastic tree model
and a language model to produce a linearized string from an unordered,
unlabeled dependency graph.
%
The first step to producing a linearized string is to assign each item
an appropriate \emph{supertag}---a fragment of a parse tree with a
leaf left open for a lexical item---using a stochastic model.
%
This process involves applying a learned model to make predictions for
the syntax of each item and then searching over the predictions to
find a consistent assignment for the entire sentence.
%
The resulting assignments allow for many possible surface realization
outputs because they can underdetermine the order and attachment of
adjuncts.
%
To finish the linearization, a language model is used to select the
most likely surface form from among the alternatives.
%
While improving the language model would improve the linearized string,
this work focuses on more accurately predicting the correct supertags from
unlabeled dependency trees.

Our work exploits deep learning to improve the model of supertag
assignment in two ways.
%
First, we analyze the use of embedding technques to generalize across
supertags.  
%
Neural networks offer a number of architectures that can cluster tree
fragments during training; such models learn to treat related structures
similarly, and we show that they improve supertag assignments.
%
Second, we analyze the use of tree recurrences to track hierarchical
relationships within the generation process.
%
Such networks can track more of the generation context than a simple
feed-forward model; as a side effect, they can simplify the problem of
computing consistent supertag assignments for an entire sentence.
%
We evaluate our contributions in two ways: first, by varying the
technique used to embed supertags, and then by comparing a
feed-forward model against our recurrent tree model.

Our presentation begins in \ref{sec:tree} with an introduction to tree
grammars and a deterministic metholodogy for inducing the elementary
trees of the grammar.
%
Next, \ref{sec:neural} presents the techniques we have developed to
represent a tree grammar using a neural architecture.
%
Then, in \ref{sec:models}, we describe the specific models we have
implemented and the algorithms used to exploit the models in NLG.
%
The experiments in \ref{sec:expt} demonstrate the improvement of the
model over baseline results based on previous work on stochastic
surface realization.
%
We conclude with a brief discussion of the future potential for neural
architectures to predict NLG choices.

\section{Tree Grammars}
\label{sec:tree}

Broadly, tree grammars are a family of tree rewriting formalisms that
produce strings as a side effect of composing primitive
hierarchical structures.  The basic syntactic units are called
elementary trees; elementary trees combine using tree-rewrite rules to
form derived phrase structure trees describing complex sentences.
Inducing a tree grammar involves fixing a formal inventory of
structures and operations for elementary trees and then inferring
instances of those structures to match corpus data.

\subsection{Grammar Formalism}

The canonical tree grammar is perhaps lexicalized tree-adjoining
grammar (LTAG) \cite{joshi1991tree}.
%
The elementary trees of LTAG consist of two disjoint sets with
distinct operations: initial trees can perform substitution operations
and auxiliary trees can perform adjunction operations.
%
The substitution operation replaces a non-terminal leaf of a target
tree with an identically-labeled root node of an initial tree.
%
The adjunction operation modifies the internal structure of a target
tree by expanding a node identically-labeled with the root and a
distinguished foot note in the auxiliary tree.
%
The lexicalization of the the grammar requires each elementary tree to
have at least one lexical item as a leaf.

LTAG incurs computational costs because it is mildly context-sensitive
in generative power.
%
Several variants reduce the complexity of the formalism by limiting
the range of adjunction operations.
%
For example, the Tree Insertion Grammar allows for adjunction as long as it is either a left or right auxiliary tree \cite{Schabes1995}.
%
Tree Substitution Grammars, meanwhile, allow for no adjunction and
only substitutions \cite{cohn2009inducing}.
%
We adopt one particular restriction on adjunction, called
sister-adjunction or \emph{insertion}, which allows trees to attach to
an interior node and add itself as a first or last child
\cite{chiang2000statistical}.
%
Chiang's sister-adjunction allows for the flat structures in
the Penn Treebank while limiting the formalism to context-free power.

\subsection{Grammar Induction}

In lexicalized tree grammars, the lexicon and the grammatical rules
are one and the same.
%
The set of possible grammatical moves which can be made are simultaneously the set of possible words which can be used next.
%
This means inducing a tree grammar from a dataset is a matter of inferring the set of constructions in the data.

%%%%\tabularnewline

We follow previous work in using bracketed phrase structure corpora
and deterministic rules to induce the grammar
\cite{bangalore2001impact,chiang2000statistical}.
%
Broadly, the methodology is to split the observed trees into the
constituents which make it up, according to the grammar formalism.
%
We use head rules
\cite{chiang2000statistical,collins1997three,magerman1995statistical}
to associate internal nodes in a bracketed tree with the lexical item
that owns it.
%
We use additional rules to classify some children as complements,
corresponding to substitution sites and root notes of complement
trees; and other children as adjuncts, corresponding to insertion
trees that combine with the parent node, either to the right or to the
left of the head.  This allows us to segment the tree into units of
substitution and insertion.%
\footnote{One particular downside of deterministically constructing
  the grammar this way is that it can produce an excess of superfluous
  elementary trees.
%
We minimize this by collapsing repeated projections in the treebank.
%
Other work has provided Bayesian models for reducing grammar
complexity by forcing it to follow Dirichlet or Pitman-Yor processes
\cite{Cohn2010}---an interesting direction for future work.}



\begin{figure*}[tH!]
\centering
\includegraphics[width=\textwidth]{spineembed.pdf}
\caption{Embedding supertags using convolutional neural networks. In (A), a tree is encoded by its features and then embedded.
 In (B), convolutional layers are used to encode the supertag into a vector.}
 \label{fig:spineembedding}
\end{figure*}

\section{Neural Representations}
\label{sec:neural}

The grammar induction of \ref{sec:tree} allows us to constrcut an
inventory of supertags to match a corpus.  For NLG, we also need to
predict how likely a supertag is to fit a specific lexical item in the
context of a generation task.  We approach this problem using neural
networks. 
%
In particular, this work makes two contributions to improve stochastic
tree modeling with neural networks.
%
First, we represent supertags as vectors through embedding techniques
that enable to model to generalize naturally over complex, but related
structures.
%
Second, we address the hierarchical dependence between choices using a
recurrent tree network that can capture long-distance influences as
well as local ones.
%
We now describe these representations in more detail.

\subsection{Embedding Supertags}

Different supertags for the same word can encode differences in the
item's own combinatory syntax, differences in argument structure, and
differences in word order.  Accordingly, words have many related
supertags, with substantial overlaps in structure, and, presumably,
corresponding similarities in their patterns of occurrence.  A
traditional machine learning approach to supertag prediction would
treat individual supertags as atoms for classification; generalizing
across supertags would require linking model parameters to handcrafted
features or backoff categories.

By contrast, neural techniques work by embedding such tokens into a
vector space. This process learns an abstract representation of tokens
that clusters similar items together and makes further predictions
as a function of those items' learned features.  The resulting
abilities to generalize across sparse data seems to be one of the most
improtant reasons for the success of deep learning in NLP.

The simplest way to embed supertags is simply to treat each structure
as distinct token that indexes a corresponding learned vector.  This
places no constraints on the learned similarity function, but it also
ignores the hieararchical structure of the elementary trees
themselves.  Previous work on deep learning with tree structures has
suggested that convolutional coding can produce embeddings that
directly exploit similarities in tree structure.  Thus, we developed
analogous techniques to encode supertags based on their underlying
tree structure.  In partiuclar, to embed a supertag, we embed each
node, group the resulting vectors to form a tensor, and then summarize
the tensor into a single vector using a series of convolutional neural
networks.

Note that each elementary tree is a complex structure with nodes
labeled by category and assigned a role that enables further tree
operations.
%
The root node's role represents the overall action associated with
that elementary tree---either substitution or insertion.
%
The remaining nodes either have the substitution point role or the
spine role---they are along the spine from root to the lexical
attachment point, and thus provide targets for further insertion.

The nodes of a supertag can be independently embedded and combined to form a
tensor of embeddings.
%
Specifically, symbols representing the syntactic category and node roles are treated as
distinct vocabulary tokens, mapped to integers, and used to retrieve a vector
representation that is learned during training.
%
The vectors are grouped into a tensor by placing the root node into the
first cell of the first row and left-aligning the decendents in the subsequent rows.
%
The two tensors are combined by concatenating along the embedding dimension.
%
This embed-and-group method is shown in on the left in Figure \ref{fig:spineembedding}.
%the root node is in first cell in the first row and the subsequent nodes are


Using a series of convolutional neural networks which learn their weights during training, the tensor of embeddings can be reduced to a single vector.
%
To reduce the tensor to a vector, the convolutions are designed with increasingly larger filter sizes.
%The convolutions are applied with increasingly larger filter sizes, leading to a reduction in dimensions.
%
Additionally, the dimensions are reduced alternatingly to also facilitate the capture of features.
%
The entire process is summarized in Eq. \ref{eq:embed} with $\Lambda$ representing the supertags, $G$ representing embedding matrices, and
$C$ representing the convolutional neural network layers.
%
Specifically, $G_s$ is the syntactic category embedding matrix and
$G_r$ is the node role embedding matrix.
%
Each convolutional layer $C$ is shown with its corresponding height and width as $C^{i,j}$.
%
The encoding first constructs the tensor, $T_\Lambda$, through the embed-and-group method.
%
Then, the embedding matrix $G_\Lambda$ is summarized from $T_\Lambda$ using the
series of convolutional layers.

\begin{align}
& T_\Lambda = [G_s(\Lambda_{syntactic~category});~G_r(\Lambda_{role})] \nonumber \\
& G_\Lambda = C^{4,5}(C^{3,1}(C^{1,3}(C^{2,1}(C^{1,2}(T_\Lambda))))) \label{eq:embed}
\end{align}



The final product, a vector per supertag, is aggregated with the other vectors
and turned into an embedding matrix.
%
This is visualized in on the right in Figure \ref{fig:spineembedding}.
%
During training and test time, supertags are input as indices and their feature
representations retrieved as an embedding.
%
Importantly, the convolutional layers are connected to the computational graph during training, so the parameters are optimized with respect to the task.


%%%%
\subsection{Recurrent Tree Networks}
\label{subsec:rtn}

\begin{figure*}[tH!]
\centering
\includegraphics[width=\textwidth]{rtn.pdf}
\caption{A recurrent tree network. (A) The dependency structure as a tree.  (B) the dependency structure as a sequence.}
 \label{fig:rtn}
\end{figure*}

Our models predict supertags as a function of the target word and its
context.  Neural networks make it possible to generalize over such
contexts by learning to represent them with a hidden state vector that
aggregates and clusters information from the relevant history.  Our
approach is to do this using a recurrent tree network.
%
While recurrent neural networks normally use the previous hidden state in the sequential order of the inputs,
%require  the hidden state to be from the
%temporally previous hidden state,
recurrent tree networks use the hidden state from the parent.
%
Utilizing the parent's hidden state rather than the sequentially previous hidden
state, the recurrent connection can travel down the branches of a tree.
%
An example of a recurrent tree network is shown in eFigure \ref{fig:rtn}.



In our recurrent tree network, child nodes gain access to a parent's hidden state
through an internal \emph{tree state}.
%
During a tree recurrence, the nodes in the dependency graph are enumerated in a
top-down traversal.
%
At each step in the recurrence, the resulting recurrent state is stored in the
tree state at the step index.
%
Descendents access the recurrent state using a topological index that is passed in as data.

The formulation is summarized in Equation \ref{eq:rnnsetup}.
%
The input to each time step in the current tree is the data, $x_t$, and a topological
index, $p_t$.
%
The recurrent tree uses $p_t$ to retrieve the parent's hidden state, $s_p$, from the
tree state, $S_{tree}$, and applies the recurrence function, $g(\dot)$.
%
The resulting recurrent state is the hidden state for child node, $s_c$.
%
The recurrent state $s_c$ is stored in the tree state, $S_{tree}$, at index $t$.

\begin{align}
       s_c~&=~RTN(x_t,p_t) \nonumber \\
           &=~g(x_t,S_{tree}[p_t]) \nonumber \\
           &=~g(x_t, s_p) \nonumber \\
S_{tree}[t]&=~s_c \label{eq:rnnsetup}
\end{align}

The use of topological indices allows for many recurrent tree networks to be
run in parallel on a GPU, increasing the efficiency of the implementation.
%
The primary concern for running parallel GPU computations is homogeneity because
the same operation will be applied to the entire data structure.
%
Normally, tree operations require flow control, making homogeneity impossible.
%
However, using topological indices and a tree state eliminates the need for flow control
by creating locally linear, homogenous computations.

\section{Models}
\label{sec:models}

To analyze the representations we describe in \ref{sec:tree} and
\ref{sec:neural}, we developed two alternative architectures for
predicting supertags in context.
%
The first is a feed-forward neural network designed to solve a closely
analogous task to the supertagging step of
\newcite{bangalore2000exploiting}'s original FERGUS model.
%
We call it Fergus-N (for Neuralized).
%
The second uses a recurrent tree network to model the generation
context.
%
Because it has this richer context representation, it takes advantage
of a slightly different chracterization of the supertag prediction
problem to streamline the problem solving involved in using the model.
%
We call this Fergus-R (for Recurrent).

For both stochastic tree models, a recurrent neural network language model is used to complete the linearization task.
%Both models produce partial results which a recurrent neural network language model is used to complete the linearization task.
%
%It is important to note that it is the same language model
%By using the same language model to finishing linearizing the output of both models, any observed differences in performance can be localized to the performance of the stochastic tree models.
%
The same language model is to eliminate the confound of
language model performance and measure performance differences in the
stochastic tree modeling.
%

\subsection{Model 1: Fergus-N}

Fergus-N is a stochastic tree model which uses local parent-child information as inputs to a feed-forward network.
%
Each parent-child pair is treated as independent of all others.
%
The probability of the parent's supertag is predicted using an embedding of the pair's lexical material and an embedding of the child's supertag.
%
(Our experiments compare the different embedding options surveyed in
\ref{sec:neural}.) 
%
Training maximizes the likelihood of the training data according to
the model.
%
Formally, our objective is to minimize the negative log probability of
the observed parent supertags for each parent-child pair, as formally
defined in Eq. \ref{eq:fergusn_obj}.
%
\begin{align}
&min_{\theta} -[\sum_p\sum_{p\to c} log[P_\theta(tag_{p} | lex_{p}, lex_{c}, tag_{c})] + \sum_c log[P_\theta(tag_{c} |lex_{p}, lex_{c})]] \label{eq:fergusn_obj}
\end{align}
%
Here $tag_p$ is the parent
supertag, $tag_c$ is the child supertag, $lex_p$ is the parent's
lexical material, and $lex_c$ is the child's lexical material.
%
Note that the probability of supertags for the leaves of the tree are computed
with respect to their parent's lexical material.

The model is implemented as a feed-forward neural network.
%
Equation \ref{eq:fergusn} details the model formulation.
%
The lexical material, $lex_p$ and $lex_c$, are embedded using the word embedding matrix, $G_w$, concatenated, and mapped to a new vector, $\omega_{lex}$, with a fully connected layer, $FC_1$.
%
The child supertag, $tag_c$, is embedded with $G_\Lambda$ and concatenated the lexical vector, $\omega_{lex}$, forming an intermediate vector representation of the node, $\omega_{node}$.
%
The node vector is repeated for each of the parent's possible supertags, $tagset_p$, and then concatenated with their embeddings to construct the set of treelet vectors, $\Omega_{treelet}$.
%
The vector states for the leaf nodes are similarly constructed, but instead combine the lexical vector, $\omega_{lex}$ with the embeddings of the child's possible supertags, $tagset_c$.
%
The final operation induces a probability distribution over the treelet and leaf vectors using a score computed by the vectorized function, $\Psi_{predict}$, as the scalar in a softmax distribution. 

\begin{align}
&\omega_{lex} = FC_1([G_w(lex_p); G_w(lex_c)]) \label{eq:fergusn} \\
&\omega_{node}=concat([G_\Lambda(tag_c);~\omega_{lex}]) \nonumber \\
&\Omega_{treelet} = concat([repeat(\omega_{node}),~G_\Lambda(tagset_p)]) \nonumber \\
&\Omega_{leaf} = concat([repeat(\omega_{lex}),~G_\Lambda(tagset_c)]) \nonumber \\
&P_\theta(tag_{p,i} | lex_{p}, lex_{c}, tag_{c})=
\frac{exp(\Psi_{predict}(\omega_{treelet_i})))}
{\sum_{j \in |tagset_p|} exp(\Psi_{predict}(\omega_{treelet_j})))} \nonumber \\
&P_\theta(tag_{c,i} |lex_{p}, lex_{c}) = 
\frac{exp(\Psi_{predict}(\omega_{leaf_i})))}
{\sum_{j \in |tagset_c|} exp(\Psi_{predict}(\omega_{leaf_j})))} \nonumber 
\end{align}


At generation time, we are given a full dependency tree.  A decoding
step is necessary to compute a high probability
assignment for all supertags simultaneously.
%
There are two primary difficulties that arise in this computation.
%
First, the conditional relationship of parents on children implies that the probability for the root supertag depends on the supertags of the entire tree.
%
Second, while it is easy to maintain local consistency---matching the syntactic
category of the child to an appropriate node on the parent---two children may choose substitution supertags which assume attachment to the same position on the parent.

To efficiently decode the supertag classifications, we implement an A* algorithm to incrementally select consistent supertag assignments.
%
At each step, the algorithm uses a priority queue to select subtrees based on their inside-outside scores.
%
The inside score is computed as the sum of the log probabilities of the supertags in the subtree.
%
The outside score is the sum of the best supertag for nodes outside the subtree, similar to \newcite{lewis2014improved}.
%
Once selected, the subtree is attached to the possible supertags of its parent that are both locally consistent and consistent among its already attached children.
%
These resulting subtrees are placed into the priority queue and the algorithm iterates to progress the search.
%
The search concludes either when a single complete tree has been found.\footnote{Although, the data has some noise so that sometimes there is no complete tree that can possibly be formed}

\subsection{Model 2: Fergus-R}

Fergus-R is a stochastic tree model implemented in a top-down recurrent tree network and augmented with soft attention.
%
For each node in the input dependency tree, soft attention---a method which learns a vectorized function to weight a group of vectors and sum into a single vector---is used to summarize its children.
%
The soft attention vector and the node's embedded lexical material serve as the input to the recurrent tree.
%
The output of the recurrent tree represents the vectorized state of each node and is combined with each node's possible supertags to form prediction states.
%
%The recurrent tree output and the embedded supertag of the node's parent are used to compute a probability distribution over supertags.
%
%Using only summarized lexical information from its ancestors and children, the model computes a probability distribution over supertags for each node's recurrent tree output.
%
%so that the soft attention and recurrence relationships are only communicating lexical information.
Importantly, removing the conditional dependence on descendents' supertags
results in the simplified objective function in Eq. \ref{eq:rtnobj} where $lex_C$ is the children's lexical information, $lex_p$ is the parent's lexical information, $tag_p$ is the supertag for the parent node, and $RTN$ is the recurrent tree network. 

\begin{align}
&min_{\theta} -[\sum_{(p,C)} P_\theta(tag_{p}|RTN,~lex_p,~lex_{C})] \label{eq:rtnobj}
\end{align}

The Fergus-R model uses only lexical information as input to calculate the probability distribution over each node's supertags. 
%
The specific formulation is detailed in Eq. \ref{eq:fergusr}.
%
First, a parent node's children, $lex_C$, are embedded using the word embedding matrix, $G_w$, and then summarized with an attention function, $\Psi_{attn}$, to form the child context vector, $\omega_{C}$. 
%
The child context is concatenated with the embedded lexical information of the parent node, $lex_p$, and mapped to a new vector space with a fully connected layer, $FC_1$, to form the lexical context vector, $\omega_{lex}$.
%
The context vector and a topological vector for indexing the internal tree state (see \ref{subsec:rtn}) are passed to the recurrent tree network, $RTN$, to compute the full state vector for the parent node, $\omega_{node}$.
%
Similar to Fergus-N, the state vector is repeated and concatenated with the vectors of the parent node's possible supertags, ${tagset}_p$, and mapped to a new vector space with a fully connected layer, $FC_2$.
%
A vector in this vector space is labeled $\omega_{elementary}$ because the combination of supertag and lexical item constitutes an elementary tree.
%
The last step is to compute the probability of each supertag using the vectorized function, $\Psi_{predict}$.

\begin{align}
&\omega_{C} = \Psi_{attn}(G_w(lex_C)) \label{eq:fergusr} \\
&\omega_{lex} = FC_1(concat(\omega_{C},~G_w(lex_p))) \nonumber \\
&\omega_{node} = RTN(\omega_{lex},~topology) \nonumber \\
&\Omega_{elementary} = FC_2(concat(repeat(\omega_{node}),~G_\Lambda(tagset_p))) \nonumber \\
&P_\theta(tag_{p,i}~|~RTN,~lex_p,~lex_{C}) = 
\frac{exp(\Psi_{predict}(\omega_{elementary_i})))}
{\sum_{j \in |\Omega|} exp(\Psi_{predict}(\omega_{elementary_j))}} \nonumber
\end{align}

Although the same A* algorithm from Fergus-N is used, the decoding for Fergus-R is far simpler.
%
As supertags are incrementally selected in the algorithm, the inside score of the subsequent subtree is computed.  
%
Where Fergus-N had to compute a incremental dynamic program to evaluate the inside score, Fergus-R decomposes into a sum of conditionally independent distributions. 
%
The resulting setup is a chart parsing problem where the inside score of combining two consistent (non-conflicting) edges is just the sum of their inside scores. 

\subsection{Linearization}

The final step to linearizing the output of Fergus-N and Fergus-R---a dependency tree annotated with supertags and partial attachment information---is a search over possible orderings with a language model. 
%
There are many possible orderings due to adjunction operation
What remains to be determined is the order in which adjuncts are attached. 
%
Following \newcite{bangalore2000exploiting}, a language model is used to select between the alternate orderings. 
%
The language model used is a two-layer LSTM trained using the Keras library on
the surface form of the Penn Treebank.
%
The surface form was minimally cleaned\footnote{With respect to the surface form, the only
cleaning operations were to merge proper noun phrases into single tokens.  Punctuation and other
common cleaning operations were not performed.} to simulate realistic scenarios.

The difficulty of selecting orderings with a language model is that the possible linearizations can grow exponentially.
%
In particular, our implementations result in a large amount of insertion trees\footnote{Many of the validation examples had more than $2^{40}$ possible linearizations.}.
%
We approach this problem using a prefix tree which stores the possible linearizations as back-pointers to their last step and the word for the current step. 
%
The prefix tree is greedily searched with 32 beams.




\section{Experiments}
\label{sec:expt}

Using the representations of \ref{sec:neural}, the models of
\ref{sec:models} can be instantiated in six different ways.  We can
use a feed-forward Fergus-N architecture or a recurrent Fergus-R
architecture.  Each architecture can embed supertags \emph{minimally},
by a simple one-hot vector; \emph{atomically}, by learning an
embedding vector corresponding to each supertag; or
\emph{structureally}, by using a convolutional coding over the
supertag tree structure.

\subsection{Training}

We trained six such models using a common experimental plantform.  We
started from the Wall Street Journal sections of the Penn Treebank,
which have been previously used for evaluating statistical tree
grammars \cite{chiang2000statistical}\footnote{A possible additional
  data source, the data from the 2011 Shared Task on Surface
  Realization, was not available}.
%
Our data pipeline breaks each sentence in the treebank into component
elementary trees and then represents the sentence in terms of a
derivation tree, specifying the tree-rewriting operations required to
construct the actual treebank surface tree from the basic supertags.
%
Abstracting syntactic information from the derivation tree leads to
the unlabeled dependency trees our models assume as input.

From this input, we extracted the atomic supertag prediction instances
and trained a network defined by each of the architectures of
\ref{sec:models} and each of the supertag representations of
\ref{sec:neural}.  As always, we used Section XXX for development,
Sections XXX for training, and Section XXX for testing.  A complete
description of network organization and training parameters is given
in the appendix.  Our code is available XXX.

\subsection{Performance Metrics}

We evaluate the performance of the models in several ways.  First, we
look at the accuracy of the supertag predictions directly output by
each model.  Second, we look at the accurace of the final supertags
obtained by decoding the model predictions to the best-ranked
consistent global assignment.  These metrics directly assess the
ability of the models to successfully learn the target distributions.

Next, we evaluate the models on the full NLG task, including
linearization.  
%
The linearization tasks licenses more freedom in supertag
classifications because it may have the same root and leaf syntactic
categories but differ in minor constructions, such as number of
arguments.
%
The freedom allows for models to be more generalized and less fit to
the specific supertags, but at the same time, it also mutes the
distinctions between classification decisions.
%
We report XXXMEASURE, following \cite{}.
%
They argue that their measure correlates with NLG performance better
than n-gram counting methods.
%
Since linearization uses a beam search, we report statistics both for
the top-ranked beam and for the empirically based beam among the
candidates computed during search.
%
The difference gives an indication of the effect of the language model
in guiding the decisions that remain after supertagging.

Finally, we report statistics about the run time of different
generation steps.
%
This allows us to assess the complexity of the different decoding
steps involved in generation, to reveal any tradeoffs among the models
between speed and accuracy.

\subsection{Results}
\label{sec:results}

\begin{table}
\centering
\begin{tabular}{|l|p{3cm}|c|c|c|}
\cline{3-4}
\multicolumn{2}{}{}& \multicolumn{2}{ |c| }{ \textbf{Accuracy}}&\multicolumn{1}{}{} \\ \hline
\textbf{Model} & \textbf{Embedding}  & \textbf{Raw Model} 
& \textbf{After Decoding} & \textbf{Running Time} \\ \hline
Fergus-N &  Structural  &  58.17\% & 57.40\%  & 1.97s \\ \cline{2-5}
         &  Atomic      &  60.69\% & 55.56\% & 1.81s \\ \cline{2-5}
         &  Minimal     &  52.09\% & 54.18\% & 2.02s \\ \cline{2-5}
\hline
Fergus-R &  Structural &  67.62\% & 57.04\% & 0.30s \\ \cline{2-5}
         &  Atomic     &  82.65\% & 62.73\% & 0.36s\\ \cline{2-5}
         &  Minimal    & 10.13\% & 19.66\% & 0.54s \\ \cline{2-5}
\hline
\end{tabular}
\caption{For each supertag and embedding pair, the mean accuracy of
  supertag classification directly output by the model and in the
  consistent global assignment output by A* decoding. Also shown is
  the median running time---which includes model computation and A*
  search.}
\label{table:accresults}
\end{table}

Table \ref{table:accresults} shows the results of supertag
prediction.
%
All differences between model are significant using a Paired-Sample t-test ($p<10^{-5}$)
%
The flexible embedding methods perform consistently better,
suggesting that the clustering capabilities of neural methods is a
crucial part of their effectiveness.
%
The convolution encoding seems to work better with Fergus-N.  
%
This merits further investigation: it might be because Fergus-N
predicts one supertag as a function of another, and so the
compositional relationships among the two trees matters more---or
because Fergus-R's contextualized decisions depend on similarities
among supertags (involving argument structure or information
structure) that are difficult for the convolutional coding to
represent or learn.

The overall best results come from Fergus-R, suggesting that it is
worthwhile to take additional context into account in this task.
%
At the same time, the median time taken to classify and decode a
sentence with Fergus-R is just one sixth that of Fergus-N.
%
We suspect that there is a general lesson in this speedup: because
neural models can be more flexible about the information they take
into account in decisions, it's especially important in designing
neural architectures to break a problem down into decisions that can
be combined easily.

Finally, decoding the network generally leads to lower accuracy.
%
It seems that our models are not doing a good job of using the
predictions it makes to triangulate to accurate and consistent
supertags. 
%
This suggests that the models could be improved by taking more or
better information into account in decoding.

\begin{table}
\centering
\begin{tabular}{|l|p{3cm}|p{2.5cm}|r|}
\cline{3-4}
\multicolumn{2}{}{} & \multicolumn{2}{|c|}{Accuracy}   \\ \hline
\textbf{Model} & \textbf{Embedding}  & \textbf{Top Scoring} & \textbf{Best Performance} \\ \hline
Fergus-N & Convolution & 65.80\% & 72.58\% \\ \cline{2-4}
         & Token       & 65.52\%  & 71.82\% \\ \cline{2-4}
         & Minimal Token & 63.79\% & 71.09\% \\ \cline{2-4}
\hline
Fergus-R & Convolution & 68.22\% & 74.70\% \\ \cline{2-4}
         & Token       &  69.29\% & 75.56\% \\ \cline{2-4}
         & Minimal Token &  58.23\% & 65.04\% \\ \cline{2-4}
\hline
\end{tabular}
\caption{Shown above as accuracy is the percentage of tokens in the linearized strings that are in correct positions according to an edit distance measure.}
\label{table:linresults}
\end{table}

Figure~\ref{table:linresults} shows the NLG evaluation results for the
different models.
%
The performance confirms our expectation that differences in supertag
accuracy after decoding correlate with NLG accuracy overall, but that
differences in NLG performance are attenuated.
%
We note by comparison that Bangalore and Rambow report an accuracy of
74.9\% in their best evaluation of FERGUS---on a data set of just 100
sentences with an average lenth of XXX.
%
Our evaluation, on XXX sentences with an average length of XXX, is much
more strenuous.

\section{Related Work}
\label{sec:relatedwork}

There are several lines of related work which explore stochastic tree models, ranging exploration with recurrent neural networks to agglomerative pairwise associations. 
%
Most similar to ours, is the top down tree structure of \newcite{zhang2016top}.
%
In this model, first, two long short-term memory (LSTM) networks are used to make branch exploration decisions, and then an additional two LSTM networks are used to continue along these branches. 
%
The work of \newcite{Tai2015} similarly use an LSTM, but instead work from the leaves upward to merge the hidden states of children inside the recurrent step. 
%
The upward merging of children to form representations for parents has also been studied as pairwise agglomerations using recursive neural networks \cite{Socher2010}.

A growing body of work investigates modeling long distance relationships using efficient stacks in modern neural architecture. 
%
\newcite{dyer2015transition} utilize an indexable LSTM as a neural stack for transition-based parsing.  
%
Similarly, \newcite{bowman2016fast} utilize both a stack and buffer for sentence comprehension by utilizing an indexable structure and performing computations with indices to these structures. 
%
Finally, recent work models long distance relationships using the dynamic composability in ApolloCaffe, a fork of the Caffe library \cite{jia2014caffe}, to build different network structures based on parse trees \cite{Andreas2016LearningTC}.

\section{Conclusion}
\label{sec:conclusion}

While many neural models of grammar have focused on parsing, there has been relatively less attention on generation.
%
This is in large part due to the different kinds of decisions that have to be made in order for generation to work. 


The models outlined in this paper push forward methodologies for modeling tree grammars in neural networks. 

A lot of work has to be done to take advantage of deep learning and translate that to tasks beyond parsing, especially language production tasks.

This is a first step in a larger research program of stochastic generation models with the kind of generalization that deep learning techniques like word embeddings and recurrent neural networks give you. 

\section*{Acknowledgments}

This research was supported in part by NSF IIS-1526723 and by a
sabbatical leave from Rutgers to Stone.

\bibliography{coling2016}
\bibliographystyle{acl}


\newpage
\appendix

\section{Appendix}
\label{sec:appendix}


\begin{table}[t!]
\centering
\begin{tabular}{lr}
\begin{tabular}[t]{|p{6cm}|r|}
\hline
\textbf{Model Parameter} & \textbf{Value} \\ \hline
\hline \multicolumn{2}{|c|}{\textbf{Fergus-N Parameters}} \\ \hline
Fully connected layer size & 256 \\ \hline
Batch size & 128 \\ \hline
\hline  \multicolumn{2}{|c|}{\textbf{Fergus-R Parameters}} \\ \hline
Fully connected layer size & 256 \\ \hline 
Hidden state size & 128 \\ \hline
Batch size & 16 \\ \hline
\hline \multicolumn{2}{|c|}{\textbf{Embedding Parameters}} \\ \hline
Convolution filter size & 48 \\ \hline
Syntactic category embedding size & 32 \\ \hline
Node role embedding size & 32 \\ \hline
Word embedding size \newline
\cite{pennington2014glove} & 300 \\ \hline
\end{tabular}
&
\begin{tabular}[t]{|p{6cm}|r|}
\hline
\textbf{Model Parameter} & \textbf{Value} \\ \hline
\hline \multicolumn{2}{|c|}{\textbf{Language Model Parameters}} \\ \hline
Hidden state size & 368 \\ \hline
Batch size & 32 \\ \hline
\hline \multicolumn{2}{|c|}{\textbf{Optimization Parameters}} \\ \hline
Optimization Algorithm & ADAM \\ \hline
Fergus-R and Fergus-N Learning Rate & 1e-4 \\ \hline
Language Model Learning Rate & 0.01 \\ \hline
Fully-Connected Dropout Rate & 0.5 \\ \hline
Recurrent Weight Dropout Rate & 0.2 \\ \hline
$L_2$ Weight Decay & 1e-6 \\ \hline
Max gradient norm & 10.0 \\ \hline
Gradient clip threshold & 5.0 \\ \hline
\end{tabular}
\end{tabular}
\caption{The parameters for the Fergus-R, Fergus-N, and language models.  The
exact specifications in configuration files can be found in the code repository that accompanies this paper.}
\label{tab:params}
\end{table}

In this appendix, we describe some design decisions that took place which did not have a place in the paper but should still be reported.  In particular, this section covers the data processing, parameters used in the final model, and design decisions for the restricting the supertag distributions. 

\subsection{Data}

This work used data processed from the Wall Street Journal portion of the Treebank corpus. Specifically, section 02-21 were used for training, section 22 for development, and section 23 for testing.  The corpus was preprocessed using the Stanford Parser to fix some common issues as well as remove information extra to syntactic category.  The resulting parse trees were processed.  They were annotated with head rules that marked the head at each node in the parse tree and which nodes were dependents or adjuncts.  These annotated trees were split at adjunction and substitution positions to form the grammar.  For the dependency structures used in the models, we use the derivation tree formed by the grammar for each parse tree.  The derivation structures were further processed to parent-child pairs and top-down traversals for each of the models.  The preprocessing and processing code are distributed with this paper. 

\subsection{Model parameters}

All of the models were implemented in the Keras \cite{chollet2015keras} and Theano \cite{theano} libraries.  The specific parameters that were used are shown in Table \ref{tab:params}.  The parameters were selected by measured performance on the development portion of the dataset.  In the accompanying code repository, the full experiment parameters---including progammatic parameters controlling the experimental design---are specified in configuration files. 

\subsection{Model design}

To facilitate efficient model computation, we restricted the computation for supertag distributions and used vectorized functions.
%
The set of supertags that are retrieved for probability distribution computation are set of supertags that are seen paired with the lexical item, reducing the output distribution from over 5000 to under 500.  
%
This was implemented as a set of indices to the embedding matrix passed into the computation with the rest of the data. 
%
Future implementations will explore having the supertag subset lookup table be part of the computation.
%
While there are several ways the affinity between supertag and lexical state vector could have been computed, we chose to concatenate the vectors, map them to a new space using a fully connected layer, and compute a score with a vectorized function.
%
The vectorized function operation is the same mechanism which calculates the probability distribution used in soft attention. 



\end{document}


For the convolutional embeddings, we used 

Model specifications
- layers, learning, etc

Head to supertag mapper


The data used in our experiments was taken from the Wall Street Jounral sections of the Treebank corpus.  

For the unlabeled dependency trees, we used the derivation structures which resulted from the deterministic grammar induction (see \ref{}). 

For the Fergus-N and Fergus-R learning problems, we processed the data into their relevant tuples.

Supertags were preprocessed into two data structures: one for the convolutional coding and one to map lexical items to supertags. 
%
The supertag data structure used for convolutional coding is a three-dimensional tensor where the first dimension is the supertag index, the second dimension is depth from root, and the third is relative sibling index. 
%
The tokens for syntactic category and node role are mapped to indices, stored in the data structure, and the data structure is stored in GPU memory.

During 


in model intro:
    - learning parameters in appendix
    - used keras + theano

in conv coding maybe:
    - gpu data structure

in experiments
    - where the data comes from
