@article{Bahdanau2014NeuralMT,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={CoRR},
  year={2014},
  volume={abs/1409.0473}
}

@ARTICLE{theano,
   author = {{Theano Development Team}},
    title = "{Theano: A {Python} framework for fast computation of mathematical expressions}",
  journal = {arXiv e-prints},
   volume = {abs/1605.02688},
 primaryClass = "cs.SC",
 keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Computer Science - Mathematical Software},
     year = 2016,
    month = may,
      url = {http://arxiv.org/abs/1605.02688},
}

@inproceedings{bangalore2000evaluation,
  title={Evaluation metrics for generation},
  author={Bangalore, Srinivas and Rambow, Owen and Whittaker, Steve},
  booktitle={Proceedings of the first international conference on Natural language generation-Volume 14},
  pages={1--8},
  year={2000},
  organization={Association for Computational Linguistics}
}

@article{lewis2014improved,
  title={Improved {CCG} parsing with semi-supervised supertagging},
  author={Lewis, Mike and Steedman, Mark},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={327--338},
  year={2014}
}

@article{jia2014caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}

@article{Goldberg2015APO,
  title={A Primer on Neural Network Models for Natural Language Processing},
  author={Yoav Goldberg},
  journal={CoRR},
  year={2015},
  volume={abs/1510.00726}
}

@article{Niepert2016LearningCN,
  title={Learning Convolutional Neural Networks for Graphs},
  author={Mathias Niepert and Mohamed Ahmed and Konstantin Kutzkov},
  journal={CoRR},
  year={2016},
  volume={abs/1605.05273}
}

@article{Andreas2016LearningTC,
  title={Learning to Compose Neural Networks for Question Answering},
  author={Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein},
  journal={CoRR},
  year={2016},
  volume={abs/1601.01705}
}

@article{Henaff2015DeepCN,
  title={Deep Convolutional Networks on Graph-Structured Data},
  author={Mikael Henaff and Joan Bruna and Yann LeCun},
  journal={CoRR},
  year={2015},
  volume={abs/1506.05163}
}

@inproceedings{Kalchbrenner2014ACN,
  title={A Convolutional Neural Network for Modelling Sentences},
  author={Nal Kalchbrenner and Edward Grefenstette and Phil Blunsom},
  booktitle={ACL},
  year={2014}
}

@inproceedings{moschitti2006making,
  title={Making Tree Kernels Practical for Natural Language Learning.},
  author={Moschitti, Alessandro},
  booktitle={EACL},
  volume={113},
  number={120},
  pages={24},
  year={2006}
}

@article{ghahramani1997factorial,
  title={Factorial hidden Markov models},
  author={Ghahramani, Zoubin and Jordan, Michael I},
  journal={Machine learning},
  volume={29},
  number={2-3},
  pages={245--273},
  year={1997},
  publisher={Springer}
}

@inproceedings{dyer2015transition,
  title={Transition-Based Dependency Parsing with Stack Long Short-Term Memory},
  author={Chris Dyer and Miguel Ballesteros and Wang Ling and Austin Matthews and Noah A. Smith},
  booktitle={ACL},
  year={2015}
}

@InProceedings{zhang2016top,
  author    = {Zhang, Xingxing  and  Lu, Liang  and  Lapata, Mirella},
  title     = {Top-down Tree Long Short-Term Memory Networks},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {310--320},
  url       = {http://www.aclweb.org/anthology/N16-1035}
}

@article{Tai2015,
abstract = {A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
archivePrefix = {arXiv},
arxivId = {1503.0075},
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
doi = {10.1515/popets-2015-0023},
eprint = {1503.0075},
file = {:home/cogniton/Downloads/1503.00075v3.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
pages = {1556--1566},
title = {{Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}},
url = {http://arxiv.org/abs/1503.0075},
year = {2015}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois},
  year={2015},
  publisher={GitHub},
  howpublished={\url{https://github.com/fchollet/keras}},
}


@article{Socher2010,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-aware recursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases “decline to comment” and “would not disclose the terms” are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1{\%} on the Wall Street Journal development dataset for sentences up to length 15.},
author = {Socher, Richard and Manning, Christopher D Cd and Ng, Andrew Y Ay},
file = {:home/cogniton/Downloads/Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks.pdf:pdf},
journal = {Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop},
pages = {1--9},
title = {{Learning continuous phrase representations and syntactic parsing with recursive neural networks}},
url = {http://wuawua.googlecode.com/files/Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks.pdf},
year = {2010}
}

@article{kalchbrenner2014convolutional,
  title={A convolutional neural network for modelling sentences},
  author={Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  journal={arXiv preprint arXiv:1404.2188},
  year={2014}
}

@article{niepert2016learning,
  title={Learning Convolutional Neural Networks for Graphs},
  author={Niepert, Mathias and Ahmed, Mohamed and Kutzkov, Konstantin},
  journal={arXiv preprint arXiv:1605.05273},
  year={2016}
}


@article{bowman2016fast,
  title={A fast unified model for parsing and sentence understanding},
  author={Bowman, Samuel R and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:1603.06021},
  year={2016}
}

@article{dyer2016recurrent,
  title={Recurrent neural network grammars},
  author={Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A},
  journal={arXiv preprint arXiv:1602.07776},
  year={2016}
}

@article{Schabes1995,
abstract = {Tree insertion grammar (TIG) is a tree-basedformalism that makes use of tree substitution and tree adjunction. TIG is related to tree adjoining grammar. However, the adjunction permitted in TIG is sufficiently restricted that TIGs only derive context-free languages and TIGs have the same cubic-time worst-case complexity bounds for recognition and parsing as context-free grammars. An efficient Earley-style parserfor TIGs is presented. Any context-free grammar (CFG) can be converted into a lexicalized tree insertion grammar (LTIG) that generates the same trees. A constructive procedure is presented for converting a CFG into a left anchored (i.e., word initial) LTIG that preserves ambiguity and generates the same trees. The L,TIGcreated can be represented compactly by taking advantage of sharing between the elementary trees in it. Methods of converting CFGs into left anchored CFGs, e.g., the methods of Greibach and Rosenkrantz, do not preserve the trees produced and result in very large output grammars. For the purpose of experimental evaluation, the LTIG lexicalization procedure was applied to eight different CFGs for subsets of English. The LTIGs created were smaller than the original CFGs. Using an implementation of the Earley-style TIG parser that was specialized for left anchored LTIGs, it was possible to parse more quickly with the LTIGs than with the original CFGs.},
author = {Schabes, Yves and Waters, Richard C.},
file = {:home/cogniton/Downloads/10.1.1.14.7116.pdf:pdf},
issn = {08912017},
journal = {Computational Linguistics},
number = {4},
pages = {479--513},
title = {{Tree Insertion Grammar : A Cubic-Time , Parsable Formalism that Lexicalizes Context-Free Grammar without Changing the Trees Produced}},
volume = {21},
year = {1995}
}


@incollection{joshi1997tree,
  title={Tree-adjoining grammars},
  author={Joshi, Aravind K and Schabes, Yves},
  booktitle={Handbook of formal languages},
  pages={69--123},
  year={1997},
  publisher={Springer}
}

@article{joshi1991tree,
  title={Tree-adjoining grammars and lexicalized grammars},
  author={Joshi, Aravind K and Schabes, Yves},
  year={1991}
}


@inproceedings{chiang2000statistical,
  title={Statistical parsing with an automatically-extracted tree adjoining grammar},
  author={Chiang, David},
  booktitle={Proceedings of the 38th Annual Meeting on Association for Computational Linguistics},
  pages={456--463},
  year={2000},
  organization={Association for Computational Linguistics}
}

@inproceedings{cohn2009inducing,
  title={Inducing compact but accurate tree-substitution grammars},
  author={Cohn, Trevor and Goldwater, Sharon and Blunsom, Phil},
  booktitle={Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={548--556},
  year={2009},
  organization={Association for Computational Linguistics}
}


@inproceedings{magerman1995statistical,
  title={Statistical decision-tree models for parsing},
  author={Magerman, David M},
  booktitle={Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
  pages={276--283},
  year={1995},
  organization={Association for Computational Linguistics}
}

@inproceedings{collins1997three,
  title={Three generative, lexicalised models for statistical parsing},
  author={Collins, Michael},
  booktitle={Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics},
  pages={16--23},
  year={1997},
  organization={Association for Computational Linguistics}
}

@article{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  journal={Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014)},
  volume={12},
  pages={1532--1543},
  year={2014}
}

@inproceedings{langkilde1998generation,
  title={Generation that exploits corpus-based statistical knowledge},
  author={Langkilde, Irene and Knight, Kevin},
  booktitle={Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1},
  pages={704--710},
  year={1998},
  organization={Association for Computational Linguistics}
}

@inproceedings{bangalore2000exploiting,
  title={Exploiting a probabilistic hierarchical model for generation},
  author={Bangalore, Srinivas and Rambow, Owen},
  booktitle={Proceedings of the 18th conference on Computational linguistics-Volume 1},
  pages={42--48},
  year={2000},
  organization={Association for Computational Linguistics}
}

@inproceedings{bangalore2001impact,
  title={Impact of quality and quantity of corpora on stochastic generation},
  author={Bangalore, Srinivas and Chen, John and Rambow, Owen},
  booktitle={Proceedings of the 2001 Conference on Empirical Methods in Natural Langauge Processing, Pittsburgh, PA},
  year={2001}
}

@article{Cohn2010,
abstract = {Inducing a grammar from text has proven to be a notoriously challenging learning task despite decades of research. The primary reason for its difﬁculty is that in order to induce plausible grammars, the underlying model must be capable of representing the intricacies of language while also ensuring that it can be readily learned from data. The majority of existing work on grammar induction has favoured model simplicity (and thus learnability) over representational capacity by using context free grammars and ﬁrst order dependency grammars, which are not sufﬁciently expressive
to model many common linguistic constructions. We propose a novel compromise by inferring a probabilistic tree substitution grammar, a formalism which allows for arbitrarily large tree fragments and thereby better represent complex linguistic structures. To limit the model's complexity we employ a Bayesian non-parametric prior which biases the model towards a sparse grammar with shallow productions. We demonstrate the model's efﬁcacy on supervised phrase-structure parsing, where we induce a latent segmentation of the training treebank, and on unsupervised dependency
grammar induction. In both cases the model uncovers interesting latent linguistic structures while producing competitive results.},
author = {Cohn, Trevor and Blunsom, Phil and Goldwater, Sharon},
file = {:home/cogniton/library/cohn10b.pdf:pdf},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {Natural Language Processing},
pages = {3053--3096},
title = {{Inducing tree-substitution grammars}},
url = {http://eprints.pascal-network.org/archive/00008105/},
volume = {11},
year = {2010}
}
